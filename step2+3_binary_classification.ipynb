{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#*WhereIsWaldo* Project - Step 2\n","##Author: Ernest Vereycken"],"metadata":{"id":"i5qUcJVPQ_LN"}},{"cell_type":"markdown","source":["### **read this before running the notebook**\n","- Be carefull when running cells multiple times, this may cause additional data to be added to a variable defined in a higher cell, this may cause incorrect results.\n","- Intermediate weights and parameters are temporarily stored in model.pth (and also read back in). If you start training again on a new data split (because of randomness) without removing the saved model.pth, the weights and parameters of the previous training will still be captured in the model.\n","- This is not the final version of Step 2. I will continue working on this once the model of step 1 is retrained.\n","- When things behave strangely, use 'Disconnect and delete runtime' and try again.\n","- If you get a cuda-related error, switch to cpu and run again, this will give you a more precise error message."],"metadata":{"id":"fFC-QnRkOfE5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bu9L3y0iDKcV"},"outputs":[],"source":["import os\n","import random\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import DatasetFolder, ImageFolder\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # use GPU if available"]},{"cell_type":"markdown","source":["## First we define all classes from [**Step 1**](https://github.com/TentaTonny/WhereIsWaldo_Team18/blob/main/Pretraining_Triplet_Loss.ipynb). We need this to load pretrained model."],"metadata":{"id":"GTzchSD3DX5z"}},{"cell_type":"code","source":["class ViT(nn.Module):\n","    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):\n","        super().__init__()\n","\n","        self.patchify = Patchify(image_size, patch_size, dim)\n","        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))\n","        self.transformer = Transformer(dim, num_layers, heads, dropout = dropout)\n","        self.classifier = nn.Linear(dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patchify(x)         # (batch_size, N, dim)\n","        x = x + self.pos_encoding    # (batch_size, N, dim)\n","        x = self.transformer(x)      # (batch_size, N, dim)\n","        x = x.mean(1)                # (batch_size, dim)\n","        x = self.classifier(x)\n","        return x\n","\n","class Patchify(nn.Module):\n","    def __init__(self, img_size, patch_size, embed_dim):\n","        super().__init__()\n","        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.embed_dim = embed_dim\n","\n","        self.proj = nn.Conv2d(in_channels = 3,\n","                              out_channels = embed_dim,\n","                              kernel_size = patch_size,\n","                              stride = patch_size)\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        x = self.proj(x)   # (batch_size, dim, img_size // patch_size, img_size // patch_size)\n","        # the input to the transformer should be of shape (batch_size, num_patches, embedding dim)\n","         # (batch_size, dim, img_size // patch_size, img_size // patch_size) --> flattten --> (batch_size, dim, num_patches) --> tranpose --> (batch_size, num_patches, dim)\n","        x = x.flatten(2).transpose(1,2)\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, num_layers, heads, dropout = 0.):\n","\n","        super().__init__()\n","\n","        self.norm = nn.LayerNorm(dim)\n","        self.layers = nn.ModuleList([])\n","\n","        for _ in range(num_layers):\n","            self.layers.append(nn.ModuleList([\n","                MultiHeadAttention(dim, heads = heads, dropout = dropout),\n","                FeedForward(dim, dropout = dropout)]))\n","\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","\n","        return self.norm(x)\n","\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, dim, heads = 8, dropout = 0.):\n","        super().__init__()\n","        self.dim = dim\n","        self.heads = heads\n","\n","        self.query = nn.Linear(dim, dim)\n","        self.keys = nn.Linear(dim, dim)\n","        self.values = nn.Linear(dim, dim)\n","\n","        self.norm = nn.LayerNorm(dim)\n","\n","        self.out_proj = nn.Linear(dim, dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # N is the total number of patches\n","        B, N, C = x.shape\n","\n","        x = self.norm(x)         # (B, N, C)\n","\n","        query = self.query(x)    # (B, N, C)\n","        key = self.keys(x)       # (B, N, C)\n","        value = self.values(x)   # (B, N, C)\n","\n","        dim_head = C // self.heads\n","\n","        # Split (B,N,C) into (B, N, num_heads, dim_head) and permute heads which yields a shape of (B, num_heads, N, dim_head)\n","        # each of the heads, should have (N, dim_head)\n","        query = query.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n","        key = key.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n","        value = value.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n","\n","        # (B, num_heads, N, dim_head) with (B, num_heads, N, dim_head) --> (B, num_heads, N, N)\n","        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n","\n","        scale = dim_head ** -0.5   # (1 / sqrt(dim_head))\n","        attention_scores = attention_scores * scale\n","        attention_scores = F.softmax(attention_scores, dim = -1) # (B, num_heads, N, N)\n","        attention_scores = self.dropout(attention_scores)\n","\n","        # extract the values\n","        # (B, num_heads, N, N) matmul (B, num_heads, N, dim_head) --> (B, num_heads, N, dim_head)\n","        out = torch.matmul(attention_scores, value)\n","\n","        # (B, num_heads, N, dim_head) --> (B, N, num_heads, dim_head) --> (B, N, C)\n","        out = out.permute(0,2,1,3).flatten(2)   # or we can use .reshape(B, N, -1) rather than .flatten(2)\n","        out = self.out_proj(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, dropout = 0.):\n","        super().__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, dim * 4),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim * 4, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"rnwqDL1VD-NL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing data\n"],"metadata":{"id":"Dx-SxldsEgM1"}},{"cell_type":"markdown","source":["### Clone data directly into session.\n","\n","**path:**    /content/Hey-Waldo/\n","\n","**notes:**\n","- data will be deleted when you end session but you can just run this again.(this only takes a couple of seconds)\n","- you can also upload waldo data to drive but I think this is easier/cleaner"],"metadata":{"id":"9cRQOm1aFOil"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PSUc62DWGh1t"}},{"cell_type":"code","source":["!cd '/content/'\n","!rm -rf '/content/Hey-Waldo/'\n","!git clone https://github.com/vc1492a/Hey-Waldo.git"],"metadata":{"id":"2UsyUEjMFouC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### some auxiliary methods."],"metadata":{"id":"ToYn5mJNGi7S"}},{"cell_type":"code","source":["def show_tensor(t):\n","  plt.imshow(np.transpose(t, (1, 2, 0)))\n","\n","def waldo_percentage(dataset):\n","  return round(dataset.count_waldos() / len(dataset.labels) * 100, 2)"],"metadata":{"id":"gghKdtG-GudH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create Dataset class for waldo data\n","#### we store image paths and their corresponding labels (0 if image does not contain waldo, 1 if image contains waldo)"],"metadata":{"id":"uJKIu1zEIF5M"}},{"cell_type":"code","source":["class WaldoBinaryDataset(Dataset):\n","    def __init__(self, imgs=None, labels=None,  folder_paths=None, transform=None):\n","        self.imgs = []\n","        self.labels = []\n","        self.transform = transform\n","        self.augmentation = transforms.Compose([transforms.RandomRotation(degrees=30),  #random rotation between -30 and 30 degrees\n","                                                transforms.RandomHorizontalFlip(p=0.5),  #0.5 chance of horizontal flipping\n","                                                transforms.RandomAdjustSharpness(p=0.5, sharpness_factor=4),\n","                                                ])\n","        self.is_train = True #set this to false for testset and val set\n","\n","        if folder_paths:\n","          #(reimplement later, kan cleaner)\n","          for folder_path in folder_paths:\n","              notwaldo_path = f'{folder_path}/notwaldo'\n","              waldo_path = f'{folder_path}/waldo'\n","              #first add images with waldo\n","              for filename in os.listdir(waldo_path):\n","                file_path = os.path.join(waldo_path, filename)\n","                self.imgs.append(file_path)\n","                self.labels.append(1)\n","\n","              #add images without waldo\n","              for filename in os.listdir(notwaldo_path):\n","                  file_path = os.path.join(notwaldo_path, filename)\n","                  self.imgs.append(file_path)\n","                  self.labels.append(0)\n","\n","          #shuffle_data\n","          self.shuffle_data()\n","\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.imgs[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        #we only perform augmentation on waldos of train set\n","        if self.is_train and self.labels[idx] == 1:\n","          image = self.augmentation(image)\n","\n","        return image, self.labels[idx]\n","\n","    #shuffle imgs and labels (but preserve img-label relations)\n","    def shuffle_data(self):\n","        combined = list(zip(self.imgs, self.labels))\n","        random.shuffle(combined)\n","        self.imgs, self.labels = zip(*combined)\n","        self.imgs = list(self.imgs)\n","        self.labels = list(self.labels)\n","\n","    def count_waldos(self):\n","      return sum(self.labels)\n","\n","\n","    #static method to create subset of dataset. x included, y excluded\n","    @staticmethod\n","    def create_subset_dataset(dataset, x, y):\n","        subset_imgs = dataset.imgs[x:y]\n","        subset_labels = dataset.labels[x:y]\n","        subset_dataset = WaldoBinaryDataset([], transform=dataset.transform)\n","        subset_dataset.imgs = subset_imgs\n","        subset_dataset.labels = subset_labels\n","        subset_dataset.waldo_count = sum(subset_labels)\n","        subset_dataset.notwaldo_count = len(subset_labels) - subset_dataset.waldo_count\n","\n","        return subset_dataset\n","\n","    @staticmethod\n","    def duplicate_waldos(dataset, duplicate_factor):\n","        duplicated_images = []\n","        duplicated_labels = []\n","\n","        for i, img in enumerate(dataset.imgs):\n","            if dataset.labels[i] == 1:\n","                for _ in range(duplicate_factor):\n","                    duplicated_images.append(img)\n","                    duplicated_labels.append(1)\n","\n","        #extend original lists with the duplicated images and labels\n","        dataset.imgs.extend(duplicated_images)\n","        dataset.labels.extend(duplicated_labels)\n","        #shuffle set again because all the added waldos are at the end of dataset.\n","        dataset.shuffle_data()"],"metadata":{"id":"A8HQAgp1I-xg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Now let's load the dataset into memory"],"metadata":{"id":"lj_K_VZuJWYc"}},{"cell_type":"code","source":["#if you use drive you should change paths here\n","folder_paths = ['/content/Hey-Waldo/128', '/content/Hey-Waldo/64']\n","\n","#resize to 128x128\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor()\n","])\n","\n","waldo_dataset = WaldoBinaryDataset(folder_paths=folder_paths, transform=transform)"],"metadata":{"id":"MCAJ2l7vLBY-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split data into train-, validation- and testset\n","\n","##### these ratios are defined in the assignment"],"metadata":{"id":"oQC5DiNhLYYs"}},{"cell_type":"code","source":["train_ratio = 0.81\n","val_ratio = 0.09\n","test_ratio = 0.1\n","\n","tolerance = 1e-10\n","#we add this tolerance because of rounding errors in floating point arithmetic\n","if not(abs(train_ratio + val_ratio + test_ratio - 1) < tolerance):\n","    raise Exception(\"invalid train/val/test ratios\")\n","\n","total_length = len(waldo_dataset)\n","train_boundary = int(train_ratio * total_length)\n","validation_boundary = int((train_ratio + val_ratio) * total_length)\n","\n","#split in train test and val set\n","train_set = WaldoBinaryDataset.create_subset_dataset(waldo_dataset, 0, train_boundary)\n","val_set = WaldoBinaryDataset.create_subset_dataset(waldo_dataset, train_boundary, validation_boundary)\n","test_set = WaldoBinaryDataset.create_subset_dataset(waldo_dataset, validation_boundary, total_length)\n","\n","#disable train on test and val\n","test_set.is_train = True\n","val_set.is_train = False\n","\n","#some checks\n","print(f'TRAIN:\\nlength: {len(train_set)}. \\nTotal length: {total_length}. \\npercentage: {round(len(train_set) / total_length * 100.0, 2)}\\nwaldos: {train_set.count_waldos()}\\nwaldo percentage: {waldo_percentage(train_set)}\\n')\n","print(f'VALIDATION:\\nlength: {len(val_set)}. \\nTotal length: {total_length}. \\npercentage: {round(len(val_set) / total_length * 100.0, 2)}\\nwaldos: {val_set.count_waldos()}\\nwaldo percentage: {waldo_percentage(val_set)}\\n')\n","print(f'TEST:\\nlength: {len(test_set)}. \\nTotal length: {total_length}. \\npercentage: {round(len(test_set) / total_length * 100.0, 2)}\\nwaldos: {test_set.count_waldos()}\\nwaldo percentage: {waldo_percentage(test_set)}\\n')"],"metadata":{"id":"vKHGVbhDLXee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imbalance and Augmentation\n","As you can see, the amount of images that contain waldo is only around 1% of the total amount. When I first trained the classification model, the model just predicted 0 for every input, which resulted in a very high accuracy. Ofcourse we don't want this so we will do some data augmentation to balance out the waldo/not waldo ratio."],"metadata":{"id":"Q-R7kXmWczqN"}},{"cell_type":"code","source":["#we duplicate every waldo n times.\n","duplicate_factor = 100\n","WaldoBinaryDataset.duplicate_waldos(train_set, duplicate_factor)\n","\n","print(f'{waldo_percentage(train_set)}%')"],"metadata":{"id":"fLZ37NkSdjwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, the ratio is now somewhere around 50%. The problem now is that we got the exact same waldo image 100 times. To fix this we add some (random) transforms, in the \\_\\_getitem__() method. Now when we get images to feed to the model, the images are all slightly different. The following data augmentations are applied:\n","\n","```\n","transforms.RandomRotation(degrees=30),  \n","transforms.RandomHorizontalFlip(p=0.5),  \n","transforms.RandomAdjustSharpness(p=0.5, sharpness_factor=4),\n","```\n","\n","\n","Let's look at this.\n","\n","**note:** if the image does not contain waldo, the augmentation will not be applied. change n untill you get an image of waldo."],"metadata":{"id":"Z949ji0gfAMs"}},{"cell_type":"code","source":["#get the nth image of the training set\n","n = 6\n","\n","first_a = train_set.__getitem__(n)\n","first_b = train_set.__getitem__(n)\n","first_c = train_set.__getitem__(n)\n","img_a = first_a[0] #we only want the img (as tensor)\n","img_b = first_b[0]\n","img_c = first_c[0]\n","\n","plt.subplot(1, 3, 1)  # 1 row, 3 columns, position 1\n","plt.imshow(np.transpose(img_a, (1, 2, 0)))\n","plt.title('img a')\n","\n","# Plotting the second image\n","plt.subplot(1, 3, 2)  # 1 row, 3 columns, position 2\n","plt.imshow(np.transpose(img_b, (1, 2, 0)))\n","plt.title('img b')\n","\n","# Plotting the third image\n","plt.subplot(1, 3, 3)  # 1 row, 3 columns, position 3\n","plt.imshow(np.transpose(img_c, (1, 2, 0)))\n","plt.title('img c')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HNn14ey_q2SI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating DataLoader for each dataset\n","train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=100, shuffle=False)\n","val_loader = DataLoader(val_set, batch_size=100, shuffle=False)"],"metadata":{"id":"SVL1-Wllxout"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading the pretained model\n","again, we could just upload the file to our drive our upload manually but I think this is easier. The github repository is public so it should work.\n","(if you do something similar in your own notebook, make sure you use the 'raw' URL)"],"metadata":{"id":"gwcHR0rqx1jn"}},{"cell_type":"code","source":["!cd '/content/'\n","!rm model_128.pth_49\n","!wget https://github.com/TentaTonny/WhereIsWaldo_Team18/raw/main/models/model_128.pth_49"],"metadata":{"id":"y_fHbcVuyTpC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### We can now load the model by creating a ViT model and transfer the learned weights and parameters from the saved model"],"metadata":{"id":"eAd01AmV1CT0"}},{"cell_type":"code","source":["pretrained_model = ViT(image_size = 128,\n","            patch_size = 16,\n","            num_classes = 2,\n","            dim = 512,\n","            num_layers = 3,\n","            heads = 8,\n","            dropout = 0.1).to(device)\n","\n","checkpoint = torch.load('/content/model_128.pth_49', map_location=device)\n","pretrained_model.load_state_dict(checkpoint['model'])\n","pretrained_model.to(device)"],"metadata":{"id":"ZkvHwxJVzvye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check results of the pretrained model\n","\n","### Plot the results of the pretrained model."],"metadata":{"id":"iXOTFa3j277r"}},{"cell_type":"code","source":["train_results = []\n","labels = []\n","\n","pretrained_model.eval()\n","with torch.no_grad():\n","    for img, label in tqdm(test_loader):\n","        train_results.append(pretrained_model(img.to(device)).cpu().numpy())\n","        labels.append(label)\n","\n","train_results = np.concatenate(train_results)\n","labels = np.concatenate(labels)\n","train_results.shape\n","\n","plt.figure(figsize=(15, 10), facecolor=\"azure\")\n","for label in np.unique(labels):\n","    tmp = train_results[labels==label]\n","    plt.scatter(tmp[:, 0], tmp[:, 1], label=label)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"EZ5u-ItN3IFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Now try this model on some augmented waldo's\n"],"metadata":{"id":"hm1Jwaa34r1g"}},{"cell_type":"code","source":["#get first n waldos of train set and show them\n","waldo_counter = 0\n","i = 0\n","waldo_imgs = []\n","n = 8\n","\n","while waldo_counter < n:\n","    img, label = train_set.__getitem__(i)\n","    if label == 1:\n","        waldo_imgs.append(img)\n","        waldo_counter += 1\n","    i += 1\n","\n","\n","plt.figure(figsize=(15,8))\n","plt.axis('off')\n","\n","#plot the waldo's (if you change n make sure to also change the rows and columns)\n","for i in range(n):\n","  plt.subplot(2,4, i+1) #2 rows, 4 columns\n","  plt.imshow(np.transpose(waldo_imgs[i], (1,2,0)))\n","  plt.title(f'waldo_{i}')"],"metadata":{"id":"5mImGG1S5QLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### We now also plot results of the pretained model on the augmented waldo's."],"metadata":{"id":"eqcbbZ0-HTda"}},{"cell_type":"code","source":["pretrained_results = []\n","\n","pretrained_model.eval()\n","with torch.no_grad():\n","    for img in waldo_imgs:\n","        pretrained_results.append(pretrained_model(img.to(device).unsqueeze(0)).cpu().numpy())\n","\n","results = [arr[0].tolist() for arr in pretrained_results]\n","\n","plt.figure(figsize=(15, 10), facecolor=\"azure\")\n","for label in np.unique(labels):\n","    tmp = train_results[labels==label]\n","    plt.scatter(tmp[:, 0], tmp[:, 1], label=label)\n","\n","#add results of augmented waldo's\n","x_values, y_values = zip(*results)\n","\n","plt.scatter(x_values, y_values, color='red',label='augmented 1')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ejolZnur9Qr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model does not perform well on the augmented data. One possible cause is that the model is trained on all data and not just the train set. This will be solved later."],"metadata":{"id":"QucDCV_zJFmy"}},{"cell_type":"markdown","source":["### Anton made a new model after this. We will now have a look at this.\n"],"metadata":{"id":"_vE-M-qi3vPx"}},{"cell_type":"code","source":["!cd '/content/'\n","!rm model_final.pth_12\n","!wget https://github.com/TentaTonny/WhereIsWaldo_Team18/raw/main/models/model_final.pth_12"],"metadata":{"id":"NQMBIO8x4ADu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_model = ViT(image_size = 128,\n","            patch_size = 16,\n","            num_classes = 2,\n","            dim = 512,\n","            num_layers = 3,\n","            heads = 8,\n","            dropout = 0.1).to(device)\n","\n","checkpoint = torch.load('/content/model_final.pth_12', map_location=device)\n","pretrained_model.load_state_dict(checkpoint['model'])\n","pretrained_model.to(device)"],"metadata":{"id":"2HI2sXKF4N4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_results = []\n","\n","pretrained_model.eval()\n","with torch.no_grad():\n","    for img in waldo_imgs:\n","        pretrained_results.append(pretrained_model(img.to(device).unsqueeze(0)).cpu().numpy())\n","\n","results = [arr[0].tolist() for arr in pretrained_results]\n","\n","plt.figure(figsize=(15, 10), facecolor=\"azure\")\n","for label in np.unique(labels):\n","    tmp = train_results[labels==label]\n","    plt.scatter(tmp[:, 0], tmp[:, 1], label=label)\n","\n","#add results of augmented waldo's\n","x_values, y_values = zip(*results)\n","\n","plt.scatter(x_values, y_values, color='red',label='augmented 1')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"VpRXBPSl4aKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Binary classification"],"metadata":{"id":"g2J4EnRIJymU"}},{"cell_type":"code","source":["class WaldoBinaryClassifier(nn.Module):\n","    def __init__(self, pretrained):\n","        super().__init__()\n","        self.pretrained = pretrained.to(device)  #pretrained transformer (with triple loss)\n","        self.pretrained.classifier = nn.Linear(pretrained.classifier.in_features, 1).to(device)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.pretrained(x))  #sigmoid activation for binary classification"],"metadata":{"id":"vChE6Rg8LV_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["binmodel = WaldoBinaryClassifier(pretrained_model)"],"metadata":{"id":"9SgFoe7VLaJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define training"],"metadata":{"id":"12qzmfHdL1kU"}},{"cell_type":"code","source":["learning_rate = 0.001\n","\n","bin_epochs = 20\n","print_every = 20\n","\n","bin_criterion = nn.BCELoss()  #cross entropy loss (binary)\n","bin_optimizer = torch.optim.Adam(binmodel.parameters(), lr=learning_rate)\n","#bin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(bin_optimizer, T_max=bin_epochs)\n","\n","def train(epoch):\n","    binmodel.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        targets = targets.float().unsqueeze(1)\n","        targets = targets.to(device)\n","\n","        bin_optimizer.zero_grad()\n","        outputs = binmodel(inputs)\n","        loss = bin_criterion(outputs, targets)\n","        loss.backward()\n","        bin_optimizer.step()\n","\n","        predictions = outputs.detach().cpu().numpy()\n","        predicted_labels = []\n","        #maybe I will write this cleaner later\n","        for p in predictions:\n","          if p[0] < 0.5:\n","            predicted_labels.append(0)\n","          else:\n","            predicted_labels.append(1)\n","\n","        truth_labels = [1 if x.item() == 1 else 0 for row in targets for x in row]\n","\n","        #count how much predictions are right\n","        count_equal = sum(a == b for a, b in zip(predicted_labels, truth_labels))\n","\n","        train_loss += loss.item()\n","\n","        total += targets.size(0)\n","        correct += count_equal\n","\n","        epoch_loss = train_loss/(batch_idx+1)\n","        epoch_acc = 100.*(correct/total)\n","\n","        if batch_idx % print_every == 0:\n","            print('Epoch {}/{}, Iter {}/{}, Train Loss: {:.3f}, Train Accuracy: {:.3f}'.format(epoch, bin_epochs, batch_idx, len(train_loader),\n","                                                                                   epoch_loss, epoch_acc))\n","    return epoch_loss, epoch_acc\n"],"metadata":{"id":"soVWzN-DLltc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define validation"],"metadata":{"id":"fMHbk4GpL5Kg"}},{"cell_type":"code","source":["def validate():\n","    binmodel.eval()\n","    val_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(tqdm(val_loader)):\n","            inputs = inputs.to(device)\n","            targets = targets.float().unsqueeze(1)\n","            targets = targets.to(device)\n","            outputs = binmodel(inputs)\n","            loss = bin_criterion(outputs, targets)\n","\n","            predictions = outputs.detach().cpu().numpy()\n","            predicted_labels = []\n","            #loop cleaner schrijven\n","            for p in predictions:\n","                if p[0] < 0.5:\n","                    predicted_labels.append(0)\n","                else:\n","                    predicted_labels.append(1)\n","\n","            truth_labels = [1 if x.item() == 1 else 0 for row in targets for x in row]\n","\n","            #count how much predictions are right\n","            count_equal = sum(a == b for a, b in zip(predicted_labels, truth_labels))\n","\n","            total += targets.size(0)\n","            correct += count_equal\n","            val_loss += loss.item()\n","\n","    acc = 100.*(correct/total)\n","    loss = val_loss/(batch_idx+1)\n","    print('Validation Accuracy: {:.3f}, Validation Loss: {:.3f}'.format(acc, loss))\n","    return acc"],"metadata":{"id":"8s_XM0WzLzBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### start training/validation loop"],"metadata":{"id":"KXcqEeG4MoT9"}},{"cell_type":"code","source":["best_acc = 0\n","\n","losses = []\n","accuracies = []\n","\n","for epoch in range(bin_epochs):\n","    epoch_loss, epoch_acc = train(epoch)\n","    losses.append(epoch_loss)\n","    accuracies.append(epoch_acc)\n","    #bin_scheduler.step()\n","    acc = validate()\n","    state = {'model': binmodel.state_dict(),\n","             'acc': acc,\n","             'epoch': epoch}\n","    if acc > best_acc:\n","        torch.save(state, 'model.pth')\n","        best_acc = acc"],"metadata":{"id":"PIqHjus1MtG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### testing (after the model is trained)"],"metadata":{"id":"zXqFvNTyNBSg"}},{"cell_type":"code","source":["def test():\n","    binmodel.eval()\n","\n","    results = []\n","    truth_labels = []\n","\n","\n","    with torch.no_grad():\n","        for img, label in tqdm(test_loader):\n","            results.append(binmodel(img.to(device)).cpu().numpy())\n","            truth_labels.append(label)\n","\n","    predicted_labels = []\n","    #note: I've set the batch_size for testing to 1, if you change this, the code below will no longer work\n","    for batch in results:\n","        for res in batch:\n","            predicted_labels.append(round(res[0]))\n","\n","    truth_labels = test_loader.dataset.labels\n","    err_idxs = []\n","    correct = 0\n","    correct_waldo = 0\n","    test_len = len(truth_labels)\n","    for i in range(test_len):\n","        if predicted_labels[i] == 1 and truth_labels[i] == 1:\n","          correct_waldo += 1\n","        if predicted_labels[i] == truth_labels[i]:\n","            correct += 1\n","        else:\n","            err_idxs.append(i)\n","\n","    print(f'\\naccuracy on test set: {round(correct/test_len, 3)}\\n')\n","\n","\n","test()\n"],"metadata":{"id":"_dejZQNjNWlg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0rpqD_AUrTk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 3 ###\n","Using the binary classifier for object detection"],"metadata":{"id":"H4vOmXC2rVwx"}},{"cell_type":"code","source":["from imutils.object_detection import non_max_suppression\n","import numpy as np\n","import argparse\n","import imutils\n","import time\n","import cv2\n","import os\n","import random\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import DatasetFolder, ImageFolder\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","\n","\n","WIN_STEP = 32\n","ROI_SIZE = (128,128)\n","INPUT_SIZE = (128,128)"],"metadata":{"id":"2RXPQuo2rc4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cd '/content/'\n","!rm -rf '/content/Hey-Waldo/'\n","!git clone https://github.com/vc1492a/Hey-Waldo.git"],"metadata":{"id":"aNw2DPEirgTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","from torchvision import transforms\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((1024, 2048)),  # Adjust the size as needed\n","    transforms.ToTensor()\n","])\n","\n","transformed_images = []\n","origs = []\n","\n","# Load and transform all 19 images\n","for i in range(1, 20):\n","  # Open the image using PIL\n","  image_path = folder_path + f'/{i}.jpg'\n","  image = Image.open(image_path)\n","  orig = cv2.imread(image_path)\n","  orig = imutils.resize(orig, width=2048, height=1024)\n","\n","  origs.append(orig)\n","\n","  # Apply the transform to the image to concistently work with tensors from the beginning\n","  transformed_image = transform(image)\n","  transformed_images.append(transformed_image)\n"],"metadata":{"id":"1jlfnCtWrkQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sliding_window_tensor(image, stepSize, windowSize):\n","    # image is a PyTorch tensor of shape [channels, height, width]\n","    for y in range(0, image.shape[1] - windowSize[1] + 1 , stepSize):\n","        for x in range(0, image.shape[2] - windowSize[0] + 1, stepSize):\n","            # Extract the window [channels, height, width]\n","            window = image[:, y:y + windowSize[1], x:x + windowSize[0]]\n","            yield (x, y, window)"],"metadata":{"id":"BTmgpy92rmIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_rois = []\n","image_loics = []\n","WIDTH, HEIGHT = 64, 64\n","STEP = 32\n","transform_resize_roi = transforms.Resize((128, 128))\n","\n","for image in transformed_images:\n","  # regions of interest\n","  rois = []\n","  locs = []\n","  for (x,y,window) in sliding_window_tensor(image, STEP, (WIDTH, HEIGHT)):\n","    w, h = WIDTH, HEIGHT\n","    # the model is trained on 128x128 images (this resizing processes really slows down the process)\n","    roi = transform_resize_roi(window)\n","    # windows for the classifier\n","    rois.append(roi)\n","    # append 2 coordinates used to show rectangle\n","    locs.append(((x,y), (x + w, y + h)))\n","  image_rois.append(rois)\n","  image_loics.append(locs)"],"metadata":{"id":"ij9dDgnKrrqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_results = []\n","with torch.no_grad():\n","  for image in image_rois:\n","    results = []\n","    for roi in image:\n","      prediction = binmodel(roi.unsqueeze(0).to(device)).cpu().numpy()\n","      results.append(prediction[0][0])\n","    total_results.append(results)\n",""],"metadata":{"id":"iMKKCZ2lrtjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","for i in range(0, len(origs)):\n","  im = origs[i]\n","  clone = im.copy()\n","  max_val = np.max(total_results[i])\n","  max_index = total_results[i].index(max_val)\n","  # index of max_value of results corresponds with index of locs = border box of waldo\n","  x = image_loics[i][max_index][0][0]\n","  y = image_loics[i][max_index][0][1]\n","  x2 = image_loics[i][max_index][1][0]\n","  y2 = image_loics[i][max_index][1][1]\n","  cv2.rectangle(clone, (x,y), (x2,y2), (0,255,0), 2)\n","  # show the visualization and current ROI\n","  cv2_imshow(clone)\n","  #cv2.imshow(\"ROI\", roiOrig)\n","  cv2.waitKey(0)"],"metadata":{"id":"YJj0v0G8ruBo"},"execution_count":null,"outputs":[]}]}