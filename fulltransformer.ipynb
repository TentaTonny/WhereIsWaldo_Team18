{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJcUERq5QQptUzF7vGFs4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TentaTonny/WhereIsWaldo_Team18/blob/main/fulltransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prbx7UbexOFS"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import DatasetFolder, ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # use GPU if available"
      ],
      "metadata": {
        "id": "ssLTiceLxa3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ViT**"
      ],
      "metadata": {
        "id": "vwl142njxd4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1FT2-BZ8xib6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/gdrive/MyDrive/Hey-Waldo/'"
      ],
      "metadata": {
        "id": "2KoVTedExwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patchify(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size, patch_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = nn.Conv2d(in_channels = 3,\n",
        "                              out_channels = embed_dim,\n",
        "                              kernel_size = patch_size,\n",
        "                              stride = patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)   # (batch_size, dim, img_size // patch_size, img_size // patch_size)\n",
        "        # the input to the transformer should be of shape (batch_size, num_patches, embedding dim)\n",
        "         # (batch_size, dim, img_size // patch_size, img_size // patch_size) --> flattten --> (batch_size, dim, num_patches) --> tranpose --> (batch_size, num_patches, dim)\n",
        "        x = x.flatten(2).transpose(1,2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PkdHeaUuxzmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * 4, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "mtLB3ffVx6Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, heads = 8, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.keys = nn.Linear(dim, dim)\n",
        "        self.values = nn.Linear(dim, dim)\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # N is the total number of patches\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        x = self.norm(x)         # (B, N, C)\n",
        "\n",
        "        query = self.query(x)    # (B, N, C)\n",
        "        key = self.keys(x)       # (B, N, C)\n",
        "        value = self.values(x)   # (B, N, C)\n",
        "\n",
        "        dim_head = C // self.heads\n",
        "\n",
        "        # Split (B,N,C) into (B, N, num_heads, dim_head) and permute heads which yields a shape of (B, num_heads, N, dim_head)\n",
        "        # each of the heads, should have (N, dim_head)\n",
        "        query = query.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n",
        "        key = key.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n",
        "        value = value.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)\n",
        "\n",
        "        # (B, num_heads, N, dim_head) with (B, num_heads, N, dim_head) --> (B, num_heads, N, N)\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "        scale = dim_head ** -0.5   # (1 / sqrt(dim_head))\n",
        "        attention_scores = attention_scores * scale\n",
        "        attention_scores = F.softmax(attention_scores, dim = -1) # (B, num_heads, N, N)\n",
        "        attention_scores = self.dropout(attention_scores)\n",
        "\n",
        "        # extract the values\n",
        "        # (B, num_heads, N, N) matmul (B, num_heads, N, dim_head) --> (B, num_heads, N, dim_head)\n",
        "        out = torch.matmul(attention_scores, value)\n",
        "\n",
        "        # (B, num_heads, N, dim_head) --> (B, N, num_heads, dim_head) --> (B, N, C)\n",
        "        out = out.permute(0,2,1,3).flatten(2)   # or we can use .reshape(B, N, -1) rather than .flatten(2)\n",
        "        out = self.out_proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "4sTbQHsMx7QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, num_layers, heads, dropout = 0.):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                MultiHeadAttention(dim, heads = heads, dropout = dropout),\n",
        "                FeedForward(dim, dropout = dropout)]))\n",
        "\n",
        "    def forward(self, x):class Transformer(nn.Module):\n",
        "    def __init__(self, dim, num_layers, heads, dropout = 0.):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                MultiHeadAttention(dim, heads = heads, dropout = dropout),\n",
        "                FeedForward(dim, dropout = dropout)]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        return self.norm(x)\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "0GbBNuK4x9iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patchify = Patchify(image_size, patch_size, dim)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))\n",
        "        self.transformer = Transformer(dim, num_layers, heads, dropout = dropout)\n",
        "        self.classifier = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patchify(x)         # (batch_size, N, dim)\n",
        "        x = x + self.pos_encoding    # (batch_size, N, dim)\n",
        "        x = self.transformer(x)      # (batch_size, N, dim)\n",
        "        x = x.mean(1)                # (batch_size, dim)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SD5dYFZnx_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((64, 64)),\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "#Mss hebben we andere transformers nodig voor de verschillende sets?"
      ],
      "metadata": {
        "id": "GNhyJ5WiyHml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset_64 = datasets.ImageFolder(f'{data_path}64', transform=transform_train)\n",
        "dataset_128 = datasets.ImageFolder(f'{data_path}128', transform=transform_train)\n",
        "\n",
        "# Combine datasets\n",
        "combined_dataset = torch.utils.data.ConcatDataset([dataset_64, dataset_128])"
      ],
      "metadata": {
        "id": "Kox67r9PyJa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Calculate the sizes for train, validation, and test sets\n",
        "total_data = len(combined_dataset)\n",
        "train_val_size = int(0.9 * total_data)\n",
        "test_size = total_data - train_val_size\n",
        "\n",
        "# Splitting combined_dataset into train_val_set and test_set\n",
        "train_val_set, test_set = torch.utils.data.random_split(combined_dataset, [train_val_size, test_size])\n",
        "\n",
        "# Further splitting train_val_set into train_set and validation_set\n",
        "train_size = int(0.9 * len(train_val_set))\n",
        "val_size = len(train_val_set) - train_size\n",
        "train_set, validation_set = torch.utils.data.random_split(train_val_set, [train_size, val_size])\n",
        "\n",
        "# Creating DataLoader for each dataset\n",
        "train_loader = DataLoader(train_set, batch_size=10, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)\n"
      ],
      "metadata": {
        "id": "TunHH09YyLhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('Not_waldo', 'Waldo')\n",
        "\n",
        "def imshow(inp):\n",
        "    \"\"\"Display image from a PyTorch Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    return inp"
      ],
      "metadata": {
        "id": "x87YVq4xyN3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get some random training images\n",
        "images, labels = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "#[Batch-size, channels, Hight?, width?]"
      ],
      "metadata": {
        "id": "1VAO3G7KyPYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's have a look at the labels for the first 4 loaded images\n",
        "print(labels[:4])\n",
        "# if we would like to map them to human-readble text labels\n",
        "print(', '.join([classes[labels[b]] for b in range(4)]))"
      ],
      "metadata": {
        "id": "sGTWxzG6yQrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_to_show = 4\n",
        "fig, axs = plt.subplots(1, num_images_to_show, figsize=(10,10))\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axs)\n",
        "\n",
        "for j in range(num_images_to_show):\n",
        "    axs[j].imshow(imshow(images[j]))\n",
        "    axs[j].set_title(classes[labels[j].item()])\n",
        "\n",
        "plt.subplots_adjust(wspace=0.1, hspace = 0)"
      ],
      "metadata": {
        "id": "Fb257IdtySDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(image_size = 64,\n",
        "            patch_size = 4,\n",
        "            num_classes = 2,\n",
        "            dim = 128,\n",
        "            num_layers = 3,\n",
        "            heads = 4,\n",
        "            dropout = 0.1).to(device)\n",
        "\n",
        "random_input = torch.randn(1,3,64,64).to(device)\n",
        "print(model(random_input).shape)"
      ],
      "metadata": {
        "id": "7oSejDWaySPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "25kA1ptTyUCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_every = 200\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        epoch_loss = train_loss/(batch_idx+1)\n",
        "        epoch_acc = 100.*correct/total\n",
        "\n",
        "        if batch_idx % print_every == 0:\n",
        "            print('Epoch {}/{}, Iter {}/{}, Train Loss: {:.3f}, Train Accuracy: {:.3f}'.format(epoch, epochs, batch_idx, len(train_loader),\n",
        "                                                                                   epoch_loss, epoch_acc))\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "1AXT-lWMyV42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "    loss = test_loss/(batch_idx+1)\n",
        "    print('Test Accuracy: {:.3f}, Test Loss: {:.3f}'.format(acc, loss))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "AaZptrFAyWpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss, epoch_acc = train(epoch)\n",
        "    losses.append(epoch_loss)\n",
        "    accuracies.append(epoch_acc)\n",
        "    scheduler.step()\n",
        "    acc = test()\n",
        "    state = {'model': model.state_dict(),\n",
        "             'acc': acc,\n",
        "             'epoch': epoch}\n",
        "    if acc > best_acc:\n",
        "        torch.save(state, 'model.pth')\n",
        "        best_acc = acc"
      ],
      "metadata": {
        "id": "kNUzheFIyZlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train loss')"
      ],
      "metadata": {
        "id": "-Jz8v31eybfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(accuracies)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train accuracy')"
      ],
      "metadata": {
        "id": "fEGrI-_Xye9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_images, gt_labels = next(iter(test_loader))"
      ],
      "metadata": {
        "id": "znIj-r7Jyhha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_test_image = 7\n",
        "input_test_img = test_images[index_test_image].unsqueeze(0).to(device)\n",
        "outputs = model(input_test_img)\n",
        "_, predicted = outputs.max(1)"
      ],
      "metadata": {
        "id": "pIRRI5QzyiZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(imshow(test_images[index_test_image]))\n",
        "plt.title('Predicted: {}, Ground Truth: {}'.format(classes[predicted.item()],classes[gt_labels[index_test_image].item()]))\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "KhGiyUg0ykNM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}